<<<<<<< HEAD
# -*- coding: utf-8 -*-
"""scraping_property_lisiting_and_price_history.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MbJWA9QVUHFfXq2Lgv-CykXGqm9HF1Rq
"""



"""**Install Packages**"""

#from colab import output, drive, files # specific to Google Colab
import pandas as pd
import numpy as np
import plotly.express as px
import requests
import warnings
import mysql.connector
from mysql.connector import Error
from sqlalchemy import create_engine
from pandas.core.indexes.base import is_string_dtype

# settings
warnings.filterwarnings("ignore")
pd.set_option("display.max_columns", None)

"""**Functions**"""

def get_listings(api_key, listing_url):
    url = "https://app.scrapeak.com/v1/scrapers/zillow/listing"

    querystring = {
        "api_key": api_key,
        "url":listing_url
    }

    return requests.request("GET", url, params=querystring)

def get_property_detail(api_key, zpid):
    url = "https://app.scrapeak.com/v1/scrapers/zillow/property"

    querystring = {
        "api_key": api_key,
        "zpid":zpid
    }

    return requests.request("GET", url, params=querystring)

def get_zpid(api_key, street, city, state, zip_code=None):
    url = "https://app.scrapeak.com/v1/scrapers/zillow/zpidByAddress"

    querystring = {
        "api_key": api_key,
        "street": street,
        "city": city,
        "state": state,
        "zip_code":zip_code
    }

    return requests.request("GET", url, params=querystring)

"""**Locals & Constants**"""


api_key ='f8e98c26-5047-44de-a777-c04379a7f6a3'



"""**Data**

**1. Property Listings For Sale**
"""

## zillow search url
#listing_url = "https://www.zillow.com/prosper-tx/?searchQueryState=%7B%22pagination%22%3A%7B%7D%2C%22isMapVisible%22%3Atrue%2C%22mapBounds%22%3A%7B%22north%22%3A45.64974638372341%2C%22south%22%3A27.61198105167906%2C%22east%22%3A-71.76382025000001%2C%22west%22%3A-98.83413275000001%7D%2C%22regionSelection%22%3A%5B%7B%22regionId%22%3A33547%2C%22regionType%22%3A6%7D%2C%7B%22regionId%22%3A6181%2C%22regionType%22%3A6%7D%5D%2C%22filterState%22%3A%7B%22sort%22%3A%7B%22value%22%3A%22globalrelevanceex%22%7D%2C%22ah%22%3A%7B%22value%22%3Atrue%7D%7D%2C%22isListVisible%22%3Atrue%2C%22mapZoom%22%3A5%7D"
listing_url = "https://www.zillow.com/homes/for_sale/?searchQueryState=%7B%22mapBounds%22%3A%7B%22north%22%3A40.896601543545266%2C%22east%22%3A-73.73176979892379%2C%22south%22%3A40.68137423871844%2C%22west%22%3A-74.1334574209941%7D%2C%22isMapVisible%22%3Atrue%2C%22filterState%22%3A%7B%22sort%22%3A%7B%22value%22%3A%22days%22%7D%2C%22ah%22%3A%7B%22value%22%3Atrue%7D%7D%2C%22isListVisible%22%3Atrue%2C%22mapZoom%22%3A12%2C%22customRegionId%22%3A%22a1905651baX1-CR1d5cdoyspx32d_1ec136%22%7D"
# get listings
listing_response = get_listings(api_key, listing_url)

# view count of properies returned in request
num_of_properties = listing_response.json()["data"]["categoryTotals"]["cat1"]["totalResultCount"]
print("Count of properties:", num_of_properties)

import json
x=json.dumps( listing_response.json())
#print(x)
with open('x','w') as outfile:
  outfile.write(x)

# view all listings
df_listings = pd.json_normalize(listing_response.json()["data"]["cat1"]["searchResults"]["mapResults"])
print("Number of rows:", len(df_listings))
print("Number of columns:", len(df_listings.columns))


#####################COLLECTING PRICE HISTORY DATA FROM ZILLOW########################################

df = pd.DataFrame() #Initialise a blank dataframe
#Use count to keep track on how many data fetches are done
count=0
#loop through property listing data and fetch corresponding property details data for each zpid
for zpid in df_listings['zpid']:

    if type(zpid)==str and count<3:
      zpid_=zpid

      prop_detail_response = get_property_detail(api_key, zpid_)

      df_prop = pd.json_normalize(prop_detail_response.json()['data'])

      df_price_hist = pd.DataFrame(df_prop["priceHistory"].iloc[0])
      # check if request is successful
      print("Request success:", prop_detail_response.json()["is_success"])
      df_price_hist["zpid"]=zpid_


      if df.empty:
        #Assign df to first property details data captured
        df=df_price_hist

      else:
        #Merge the rest of property details data collected with initial data
        df=df._append(df_price_hist)
        count+=1



###############################SAVING TO MYSQL##########################################

try:
    mydb = mysql.connector.connect(
        host="localhost",
        user="sa",
        password="JosHa1234!", port='3308')
    
    print("Connection established")

    cursor = mydb.cursor()

    cursor.execute("create database if not exists new_db")
    mydb.commit()
    print("Database created successfully")
    cursor.execute("use new_db")
    #cursor.close()
    #mydb.close()

except mysql.connector.Error as err:
    print("An error occurred:", err)


hostname= "localhost"
database= "new_db"
username= "sa"
password= "JosHa1234!"
prt=3308

engine = create_engine("mysql+pymysql://{user}:{pw}@{host}:{port}/{db}".format(host=hostname, db=database, user=username, pw=password,port=prt))

#Inspect  df_listing to remove columns with dictionary like data format
for col in df_listings.columns:
    if 'hdpData.homeInfo.open_house_info.open_house_showing' in col:
        del df_listings[col]
#Save property listing to mysql
df_listings.to_sql('property_listing', engine, if_exists='replace', index=False)



#Inspect  price history  to remove columns with dictionary like data format
for col in df.columns:
    if 'attributeSource' in col:
        del df[col]
    elif 'sellerAgent' in col:
       del df[col]
 #Save property listing to mysql      
df.to_sql('price_history', engine, if_exists='replace', index=False)

################################################ ###############################################################################
=======

pip install  mysql-connector-python
>>>>>>> 92ee9271c57ec74c02609075a36c889891aa084f
